import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
from config import (IMAGES_DIR, MASKS_DIR, RANDOM_SEED, N_EPOCHS, BATCH_SIZE, 
                   MAMBA_VARIANT, IMAGE_SIZE, NORMALIZE_METHOD, USE_DATA_AUGMENTATION, NUM_WORKERS,
                   MAX_SAMPLES, USE_CACHE, LOG_INTERVAL, USE_LIGHTWEIGHT_MODEL, IS_TESTING)
from model.unet_mamba_variants import UNetMamba
from utils.metrics import dice_score
from utils.plotting import create_training_summary_plot, plot_training_history
from preprocessing.dataset_v2 import HepaticVesselDatasetV2
import time
import os

def show_config():
    """Muestra la configuraciÃ³n actual de manera organizada"""
    print("="*60)
    if IS_TESTING:
        print("ğŸ§ª ENTRENAMIENTO UNet + Mamba - MODO TESTING/DESARROLLO")
        print("   (ConfiguraciÃ³n optimizada para hardware limitado)")
    else:
        print("ğŸš€ ENTRENAMIENTO UNet + Mamba - MODO PRODUCCIÃ“N")
        print("   (ConfiguraciÃ³n para entrenamiento completo)")
    print("="*60)
    print(f"ğŸ” Variante Mamba: {MAMBA_VARIANT}")
    print(f"ğŸ–¼ï¸  TamaÃ±o de imagen: {IMAGE_SIZE}x{IMAGE_SIZE}")
    print(f"ğŸ“¦ Batch size: {BATCH_SIZE}")
    print(f"ğŸ”¢ Ã‰pocas: {N_EPOCHS}")
    print(f"ğŸ¯ Max muestras: {MAX_SAMPLES}")
    print(f"ğŸ”§ NormalizaciÃ³n: {NORMALIZE_METHOD}")
    print(f"ğŸ² Data Augmentation: {USE_DATA_AUGMENTATION}")
    print(f"ğŸ’¾ Cache: {USE_CACHE}")
    print(f"âš¡ Modelo ligero: {USE_LIGHTWEIGHT_MODEL}")
    print("="*60)

def main():
    # Mostrar configuraciÃ³n
    show_config()
    
    # Verificar disponibilidad de CUDA
    print("ğŸ” Verificando dispositivos disponibles...")
    print(f"   PyTorch version: {torch.__version__}")
    print(f"   CUDA disponible: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDA version: {torch.version.cuda}")
        print(f"   NÃºmero de GPUs: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"   Memoria total: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB")
        device = torch.device("cuda")
    else:
        print("   âš ï¸  CUDA no estÃ¡ disponible. Posibles causas:")
        print("   - PyTorch instalado sin soporte CUDA")
        print("   - Drivers de NVIDIA no instalados")
        print("   - GPU no compatible")
        device = torch.device("cpu")
    
    print(f"ğŸ–¥ï¸  Dispositivo seleccionado: {device}")
    
    model = UNetMamba(mode="full", strategy="integrate").to(device)
    print(f"ğŸ” Usando modelo: UNet + Mamba ({MAMBA_VARIANT})")
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=1e-4)

    # Crear datasets separados para entrenamiento y validaciÃ³n
    print("\nğŸ“ Creando datasets...")
    if IS_TESTING:
        print(f"   ğŸ§ª Modo testing: limitando a {MAX_SAMPLES} muestras")
    else:
        print(f"   ğŸš€ Modo producciÃ³n: usando hasta {MAX_SAMPLES} muestras")
    
    train_dataset = HepaticVesselDatasetV2(
        IMAGES_DIR, MASKS_DIR, 
        image_size=IMAGE_SIZE,
        normalize_method=NORMALIZE_METHOD,
        is_training=True,  # Con augmentation
        include_empty=False,
        cache_data=USE_CACHE,
        max_samples=MAX_SAMPLES
    )
    
    val_dataset = HepaticVesselDatasetV2(
        IMAGES_DIR, MASKS_DIR, 
        image_size=IMAGE_SIZE,
        normalize_method=NORMALIZE_METHOD,
        is_training=False,  # Sin augmentation
        include_empty=False,
        cache_data=USE_CACHE,
        max_samples=MAX_SAMPLES
    )

    # Dividir Ã­ndices
    indices = list(range(len(train_dataset)))
    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=RANDOM_SEED)

    train_loader = DataLoader(Subset(train_dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)
    val_loader = DataLoader(Subset(val_dataset, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)
    
    # Mostrar estadÃ­sticas del dataset
    print(f"\nğŸ“Š EstadÃ­sticas del dataset:")
    print(f"   ğŸ“š Entrenamiento: {len(train_loader)} batches ({len(train_idx)} muestras)")
    print(f"   ğŸ” ValidaciÃ³n: {len(val_loader)} batches ({len(val_idx)} muestras)")
    
    train_stats = train_dataset.get_dataset_stats()
    for key, value in train_stats.items():
        print(f"   {key}: {value}")

    print(f"\nğŸš€ Iniciando entrenamiento...")
    start_time = time.time()
    best_dice = 0.0
    
    # Listas para guardar mÃ©tricas de entrenamiento
    train_losses = []
    val_losses = []
    train_dice_scores = []
    val_dice_scores = []
    
    # Crear directorio para outputs
    output_dir = f"outputs/{MAMBA_VARIANT}_{'testing' if IS_TESTING else 'production'}"
    os.makedirs(output_dir, exist_ok=True)
    print(f"ğŸ“ Resultados se guardarÃ¡n en: {output_dir}")
    
    for epoch in range(N_EPOCHS):
        epoch_start = time.time()
        
        # ENTRENAMIENTO
        model.train()
        train_loss, train_dice = 0, 0
        batch_count = 0
        
        print(f"\nğŸ”„ Ã‰poca {epoch+1}/{N_EPOCHS} - Entrenamiento:")
        
        for batch_idx, (img, mask) in enumerate(train_loader):
            img, mask = img.to(device), mask.to(device)
            
            # Forward pass
            output = model(img)
            loss = criterion(output, mask)
            dice = dice_score(output, mask)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            train_dice += dice.item()
            batch_count += 1
            
            # Log cada LOG_INTERVAL batches
            if (batch_idx + 1) % LOG_INTERVAL == 0:
                current_loss = train_loss / batch_count
                current_dice = train_dice / batch_count
                print(f"   Batch {batch_idx+1}/{len(train_loader)} - Loss: {current_loss:.4f}, Dice: {current_dice:.4f}")

        # VALIDACIÃ“N
        print(f"   ğŸ” Validando...")
        model.eval()
        val_loss, val_dice = 0, 0
        
        with torch.no_grad():
            for img, mask in val_loader:
                img, mask = img.to(device), mask.to(device)
                output = model(img)
                loss = criterion(output, mask)
                dice = dice_score(output, mask)
                
                val_loss += loss.item()
                val_dice += dice.item()

        # Calcular promedios
        train_loss /= len(train_loader)
        train_dice /= len(train_loader)
        val_loss /= len(val_loader)
        val_dice /= len(val_loader)
        
        epoch_time = time.time() - epoch_start
        elapsed_time = time.time() - start_time

        print(f"\n   ğŸ“Š Resultados Ã‰poca {epoch+1}:")
        print(f"      ğŸš‚ Train - Loss: {train_loss:.4f} | Dice: {train_dice:.4f}")
        print(f"      ğŸ” Val   - Loss: {val_loss:.4f} | Dice: {val_dice:.4f}")
        print(f"      â±ï¸  Tiempo Ã©poca: {epoch_time:.1f}s | Total: {elapsed_time:.1f}s")

        # Guardar mÃ©tricas para plotting
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        train_dice_scores.append(train_dice)
        val_dice_scores.append(val_dice)

        # Guardar mejor modelo
        if val_dice > best_dice:
            best_dice = val_dice
            model_path = os.path.join(output_dir, f"best_hepatic_model_{MAMBA_VARIANT}_opt.pth")
            torch.save(model.state_dict(), model_path)
            print(f"      ğŸ’¾ Â¡Nuevo mejor modelo guardado! Dice: {best_dice:.4f}")
            
        # Limpiar memoria GPU
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

    # Guardar modelo final
    final_model_path = os.path.join(output_dir, f"final_hepatic_model_{MAMBA_VARIANT}_opt.pth")
    torch.save(model.state_dict(), final_model_path)
    
    total_time = time.time() - start_time
    print(f"\nğŸ‰ ENTRENAMIENTO COMPLETADO!")
    print(f"   â±ï¸  Tiempo total: {total_time:.1f}s ({total_time/60:.1f} min)")
    print(f"   ğŸ† Mejor Dice Score: {best_dice:.4f}")
    print(f"   ğŸ’¾ Modelo final guardado: {final_model_path}")
    
    # Generar plots y visualizaciones
    print(f"\nğŸ“Š Generando visualizaciones...")
    
    # 1. Plot rÃ¡pido de mÃ©tricas de entrenamiento
    plot_path = os.path.join(output_dir, "training_history.png")
    plot_training_history(train_losses, val_losses, train_dice_scores, val_dice_scores,
                         save_path=plot_path, show_plot=False)
    
    # 2. Cargar mejor modelo para evaluaciÃ³n
    print(f"ğŸ”„ Cargando mejor modelo para evaluaciÃ³n...")
    best_model = UNetMamba(mode="full", strategy="integrate").to(device)
    best_model_path = os.path.join(output_dir, f"best_hepatic_model_{MAMBA_VARIANT}_opt.pth")
    if os.path.exists(best_model_path):
        best_model.load_state_dict(torch.load(best_model_path))
        print(f"âœ… Mejor modelo cargado desde: {best_model_path}")
        
        # 3. Crear resumen completo con predicciones del modelo
        plots_dir = os.path.join(output_dir, "plots")
        create_training_summary_plot(train_losses, val_losses, train_dice_scores, val_dice_scores,
                                   best_model, val_loader, device, save_dir=plots_dir)
    else:
        print(f"âš ï¸  No se encontrÃ³ el mejor modelo en: {best_model_path}")
    
    # 4. Guardar mÃ©tricas en CSV para anÃ¡lisis posterior
    import pandas as pd
    metrics_df = pd.DataFrame({
        'epoch': range(1, N_EPOCHS + 1),
        'train_loss': train_losses,
        'val_loss': val_losses,
        'train_dice': train_dice_scores,
        'val_dice': val_dice_scores
    })
    csv_path = os.path.join(output_dir, "training_metrics.csv")
    metrics_df.to_csv(csv_path, index=False)
    print(f"ğŸ“‹ MÃ©tricas guardadas en CSV: {csv_path}")
    
    print("="*60)
    print(f"ğŸ¯ Resumen final:")
    print(f"   ğŸ“‚ Todos los archivos en: {output_dir}")
    print(f"   ğŸ“Š Plots disponibles en: {os.path.join(output_dir, 'plots')}")
    print(f"   ğŸ’¾ Mejor modelo: {best_model_path}")
    print(f"   ğŸ“‹ MÃ©tricas CSV: {csv_path}")
    print("="*60)

if __name__ == "__main__":
    main()
