# ğŸ§  Hepatic Vessel Segmentation with UNet + Mamba Variants

Este proyecto entrena una red UNet 2D para segmentaciÃ³n de venas hepÃ¡ticas en imÃ¡genes mÃ©dicas, integrando variantes del bloque Mamba en el encoder y/o decoder.

---

## ğŸ“ Estructura del proyecto

```
structured_project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py                  # Entrena una sola variante desde config
â”‚   â”œâ”€â”€ main_variants.py         # Entrena todas ("simple", "full", "v") y grafica
â”‚   â”œâ”€â”€ plot_metrics.py          # Graficar manualmente resultados
â”‚   â”œâ”€â”€ compare_results.csv      # Tabla con mejor Dice por variante
â”‚   â”œâ”€â”€ results_*.csv            # Resultados por epoch
â”‚   â”œâ”€â”€ plots/                   # ImÃ¡genes de mÃ©tricas
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ mamba_block.py
â”‚   â”‚   â”œâ”€â”€ unet_mamba_variants.py
â”‚   â”‚   â””â”€â”€ v_mamba/             # Bloque VMamba
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ plotting.py
â”‚   â””â”€â”€ config.py
```

---

## ğŸ§ª Entorno virtual (recomendado)

### ğŸ§ WSL2 (Recomendado para Windows)
```bash
# Migrar proyecto a WSL2 (mejor soporte CUDA)
mkdir -p ~/tesis && cd ~/tesis
cp -r /mnt/c/Tesis/Modelo\ VH/src .
cp -r /mnt/c/Tesis/Modelo\ VH/data .
cp /mnt/c/Tesis/Modelo\ VH/requirements.txt .

# Instalar dependencias del sistema
sudo apt update && sudo apt install -y python3-pip python3-venv build-essential

# Crear ambiente virtual
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### ğŸªŸ Windows Nativo
```bash
python -m venv .venv
.venv\\Scripts\\activate
pip install -r requirements.txt
```

### ğŸ Linux/macOS
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸš€ Entrenamiento

### â–¶ï¸ 1. Entrenar una variante (definida en `config.py`):

```bash
python src/main.py
```

Genera:
- `hepatic_model_<variant>.pth`
- `results_<variant>.csv`

---

### â–¶ï¸ 2. Entrenar y comparar las tres variantes (simple, full, v):

```bash
python src/main_variants.py
```

Genera:
- `results_simple.csv`, `results_full.csv`, `results_v.csv`
- `compare_results.csv` con mejores mÃ©tricas
- GrÃ¡ficos: `plots/dice_score.png`, `plots/loss.png`

---

## ğŸ“Š VisualizaciÃ³n manual (opcional)

```bash
python src/plot_metrics.py
```

---

## ğŸ³ EjecuciÃ³n con Docker (Recomendado para servidores)

### ğŸ—‚ï¸ GestiÃ³n de MÃºltiples Datasets

**IMPORTANTE:** Los datasets NO se copian al contenedor Docker. Se montan como volÃºmenes externos para:
- âœ… Imagen Docker ligera (~2GB vs >50GB)
- âœ… Flexibilidad para cambiar datasets
- âœ… No duplicar datos grandes
- âœ… Resultados organizados por dataset

```bash
# Estructura recomendada
datasets/
â”œâ”€â”€ decathlon/          # Medical Segmentation Decathlon
â”‚   â”œâ”€â”€ imagesTr/
â”‚   â””â”€â”€ labelsTr/
â”œâ”€â”€ ircad/              # IRCAD dataset
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ masks/
â””â”€â”€ custom/             # Tu dataset personalizado
    â”œâ”€â”€ images/
    â””â”€â”€ masks/
```

### ğŸ—ï¸ Construir la imagen

```bash
# Construir imagen Docker (SIN datos)
docker-compose build
```

### ğŸš€ Ejecutar entrenamiento

```bash
# MÃ©todo 1: Scripts automatizados (Recomendado)
chmod +x run_dataset.sh

# Entrenar con dataset especÃ­fico
./run_dataset.sh decathlon train       # Dataset Decathlon
./run_dataset.sh ircad variants        # Dataset IRCAD - todas las variantes
./run_dataset.sh custom jupyter        # Dataset personalizado + Jupyter

# MÃ©todo 2: Docker Compose directo
docker-compose -f docker-compose.datasets.yml up hepatic-vessel-decathlon
docker-compose -f docker-compose.datasets.yml up jupyter-dev
```

### ğŸ“œ Scripts automatizados

```bash
# Linux/macOS
./run_dataset.sh [dataset] [comando]

# Windows
.\run_dataset.bat [dataset] [comando]

# Comandos disponibles:
#   train      - Entrenar una variante
#   variants   - Entrenar todas las variantes  
#   jupyter    - Abrir Jupyter Lab
#   compare    - Comparar configuraciones
#   visualize  - Generar visualizaciones
#   shell      - Abrir shell en contenedor
```

### ğŸ—‚ï¸ Persistencia de datos

Los resultados se organizan automÃ¡ticamente por dataset:
```
results/
â”œâ”€â”€ decathlon/          # Resultados dataset Decathlon
â”œâ”€â”€ ircad/              # Resultados dataset IRCAD  
â””â”€â”€ custom/             # Resultados dataset personalizado

models/
â”œâ”€â”€ decathlon/          # Modelos entrenados por dataset
â”œâ”€â”€ ircad/
â””â”€â”€ custom/
```

ğŸ“– **GuÃ­a completa:** Ver [DATASETS.md](DATASETS.md) para configuraciÃ³n detallada.

### âš™ï¸ ConfiguraciÃ³n Docker

**Variables de entorno importantes:**
- `DATASET_NAME=decathlon` - Nombre del dataset
- `IMAGES_DIR=/app/data/decathlon/imagesTr` - Ruta a imÃ¡genes
- `MASKS_DIR=/app/data/decathlon/labelsTr` - Ruta a mÃ¡scaras
- `CUDA_VISIBLE_DEVICES=0` - GPU a usar
- `PYTHONPATH=/app` - Path de Python

**Requisitos del servidor:**
- Docker >= 20.10
- docker-compose >= 1.29
- NVIDIA Docker (para GPU)
- Espacio suficiente para datasets en host

---

## âš™ï¸ ConfiguraciÃ³n rÃ¡pida

Modifica `src/config.py`:

```python
# Rutas de datos
IMAGES_DIR = "path/a/imagenes"
MASKS_DIR = "path/a/mÃ¡scaras"

# Modelo
MAMBA_VARIANT = "simple"  # o "full" o "v"
N_EPOCHS = 10
BATCH_SIZE = 4

# Preprocesamiento NUEVO ğŸ†•
IMAGE_SIZE = 256  # 224 o 256 (recomendado para segmentaciÃ³n)
NORMALIZE_METHOD = "minmax"  # "minmax", "zscore", "clahe", "percentile"
USE_DATA_AUGMENTATION = True

# Data Augmentation personalizable
AUGMENTATION_PARAMS = {
    "rotation_range": 15,        # Rotaciones Â±15Â°
    "horizontal_flip": True,     # Flip horizontal
    "zoom_range": 0.1,          # Zoom Â±10%
    "brightness_range": 0.2,     # Brillo Â±20%
    "contrast_range": 0.2,       # Contraste Â±20%
    "noise_factor": 0.05,        # Ruido gaussiano
    "elastic_deform": True,      # DeformaciÃ³n elÃ¡stica
    "cutout_prob": 0.1          # Probabilidad de cutout
}
```

---

## ğŸ”¬ ComparaciÃ³n de configuraciones

```bash
# Comparar diferentes tamaÃ±os y mÃ©todos de normalizaciÃ³n
python src/compare_preprocessing.py

# Resultados se guardan en experiments/preprocessing_comparison.csv
```

### ğŸ“ **224px vs 256px - Â¿CuÃ¡l elegir?**

**256x256 (Recomendado) âœ…:**
- Mejor para segmentaciÃ³n mÃ©dica
- Preserva mÃ¡s detalles de vasos pequeÃ±os
- Compatible con la mayorÃ­a de arquitecturas
- Buen balance velocidad/calidad

**224x224:**
- MÃ¡s rÃ¡pido de entrenar
- Menos memoria GPU
- EstÃ¡ndar en clasificaciÃ³n (ImageNet)
- Puede perder detalles finos

**RecomendaciÃ³n:** Usa **256px** para segmentaciÃ³n de vasos hepÃ¡ticos.

### ğŸ”§ **MÃ©todos de NormalizaciÃ³n**

- **MinMax** (Por defecto): Normaliza a [0,1], preserva distribuciÃ³n original
- **Z-Score**: Media=0, std=1, mejor para datos con distribuciÃ³n normal
- **CLAHE**: Mejora contraste local, excelente para imÃ¡genes mÃ©dicas
- **Percentile**: Robusta a outliers, usa percentiles 1-99

### ğŸ¯ **Data Augmentation Inteligente**

Transformaciones optimizadas para imÃ¡genes mÃ©dicas:
- âœ… Rotaciones moderadas (Â±15Â°)
- âœ… Flips horizontales
- âœ… Zoom controlado
- âœ… Cambios de brillo/contraste
- âœ… DeformaciÃ³n elÃ¡stica
- âœ… Ruido gaussiano
- âŒ Flips verticales (anatomÃ­a)
- âŒ Rotaciones extremas

---

## ğŸ“¦ Requisitos clave

```
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
matplotlib>=3.7.0
pandas>=2.0.0
scikit-learn>=1.3.0
mamba-ssm>=1.2.0
nibabel>=5.1.0
jupyter>=1.0.0
```

---

## ğŸ§  Variantes Mamba

- `simple`: bloque Mamba simple propio
- `full`: bloque Mamba original (`mamba-ssm`)
- `v`: bloque VMamba (`v-mamba`)

---

## ğŸ“Œ Notas

- Trabaja con cortes 2D (`.nii.gz`)
- Normaliza intensidades por slice
- Redimensiona todo a 256Ã—256
- Aplica validaciÃ³n cruzada (split train/val)
- Optimizado para GPU con CUDA

---

## ğŸ‘¤ Autor

Proyecto de Tesis â€“ SegmentaciÃ³n de venas hepÃ¡ticas con UNet + Mamba
